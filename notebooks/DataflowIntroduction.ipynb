{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python Dataflow\n",
    "This notebook presents step by step the process of constructing a Dataflow pipeline that reads JSON-formatted sales records from a text file and writes per product sales results to a BigQuery table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "When playing with this notebook we assume that you have the latest Python Dataflow installed from https://github.com/GoogleCloudPlatform/DataflowPythonSDK/releases\n",
    "\n",
    "For instance, to install the v0.2.2 release (currently the latest) use the download links associated with the release:\n",
    "`pip install https://github.com/GoogleCloudPlatform/DataflowPythonSDK/archive/v0.2.2.zip`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read data from an `input_file` and write results to an `output_file`. Later, we will read from a set of files and write to a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = './datain'\n",
    "output_file = './dataout'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the notebook self-contained we generate our own input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Timestamp\": 1460157827.874169, \"Price\": 1000, \"ProductName\": \"Product(0)\", \"ProductID\": 1}\r\n",
      "{\"Timestamp\": 1460157827.87439, \"Price\": 1000, \"ProductName\": \"Product(1)\", \"ProductID\": 2}\r\n",
      "{\"Timestamp\": 1460157827.874413, \"Price\": 1000, \"ProductName\": \"Product(2)\", \"ProductID\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "def generate_data(outfile, size):\n",
    "    import json\n",
    "    import time\n",
    "    with open(outfile, 'wt') as f:  \n",
    "        for _ in xrange(size):\n",
    "            for ix in xrange(10):\n",
    "                f.write('%s\\n' % json.dumps(\n",
    "                        {'ProductID': 1 + ix, 'ProductName': 'Product(%s)' % ix, \n",
    "                         'Price': 1000, 'Timestamp': time.time()}))\n",
    "\n",
    "generate_data(input_file, 100)\n",
    "!head -3 $input_file    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a standard Dataflow import statement. Most objects are in the `df` or `df.io` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.cloud.dataflow as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three steps involved in creating and running a pipeline:\n",
    "* Create the `Pipeline` object\n",
    "* Create the graph of data transforms\n",
    "* Run the pipeline graph\n",
    "\n",
    "The graph of transforms is sometimes called workflow and therefore running a workflow is the same thing as running a pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1. Create a `Pipeline` object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2. Build the graph of transforms__\n",
    "The following workflow is essentially a file copy workflow. A `Read` transform will read line by line  from a text file source and then the resulting PCollection is written using a `Write` transform to a text file sink. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PValue transform=<_NativeWrite(PTransform) label=[native_write]> at 0x10bdd0450>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p \n",
    "   |df.io.Read(df.io.TextFileSource(input_file))\n",
    "   | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3. Run the pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.dataflow.runners.runner.PipelineResult at 0x1039b4c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the output file contains the same JSON-formatted sales records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Timestamp\": 1460157827.874169, \"Price\": 1000, \"ProductName\": \"Product(0)\", \"ProductID\": 1}\r\n",
      "{\"Timestamp\": 1460157827.87439, \"Price\": 1000, \"ProductName\": \"Product(1)\", \"ProductID\": 2}\r\n",
      "{\"Timestamp\": 1460157827.874413, \"Price\": 1000, \"ProductName\": \"Product(2)\", \"ProductID\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to build a graph that is not associated with a pipeline. Later you can run it using a pipeline instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = (\n",
    "    df.io.Read(df.io.TextFileSource(input_file))\n",
    "    | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is followed by the code needed to run the graph in a pipeline and vizualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Timestamp\": 1460157827.874169, \"Price\": 1000, \"ProductName\": \"Product(0)\", \"ProductID\": 1}\r\n",
      "{\"Timestamp\": 1460157827.87439, \"Price\": 1000, \"ProductName\": \"Product(1)\", \"ProductID\": 2}\r\n",
      "{\"Timestamp\": 1460157827.874413, \"Price\": 1000, \"ProductName\": \"Product(2)\", \"ProductID\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "p | graph\n",
    "p.run()\n",
    "!head -3 $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code above used to run the workflow will be repeated a lot we will define a IPython cell magic to be used\n",
    "instead.\n",
    "\n",
    "__NOTE__ Not sure this magic adds enough value to keep it in. __TBD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "@register_cell_magic\n",
    "def dataflow_run(line, cell):\n",
    "    p = df.Pipeline('DirectPipelineRunner')\n",
    "    p | eval(cell)\n",
    "    p.run()\n",
    "    !head -3 $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply prefix the graph construction code with `%%dataflow_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Timestamp\": 1460157827.874169, \"Price\": 1000, \"ProductName\": \"Product(0)\", \"ProductID\": 1}\r\n",
      "{\"Timestamp\": 1460157827.87439, \"Price\": 1000, \"ProductName\": \"Product(1)\", \"ProductID\": 2}\r\n",
      "{\"Timestamp\": 1460157827.874413, \"Price\": 1000, \"ProductName\": \"Product(2)\", \"ProductID\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse the JSON-formatted input records. A `Read` transform using a `TextFileSource` returns a PCollection of the lines in the input file. The `parse_record` function below returns a 2-tuple (product ID, price) for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(e):\n",
    "    import json\n",
    "    r = json.loads(e)\n",
    "    return r['ProductID'], r['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above function in a ParDo transform which allows specifying element-wise custom computations for a PCollection. There are three flavors of ParDo: Map, FlatMap, and ParDo. Map and FlatMap are simplified versions that take function objects with the first argument being the element to be processed. The Map's function is a one to one mapping and the FlatMap's function is a one to many mapping (including zero). In practice, this means that FlatMap must return an iterable containing the resulting elements. We write a ParDo example a little bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\r\n",
      "(2, 1000)\r\n",
      "(3, 1000)\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.Map(parse_record)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use FlatMap instead of Map in the workflow above we will have to modify the `parse_record` function to return an iterable instead of an element since FlatMap returns zero or more elements as an iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(e):\n",
    "    import json\n",
    "    r = json.loads(e)\n",
    "    yield r['ProductID'], r['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\r\n",
      "(2, 1000)\r\n",
      "(3, 1000)\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.FlatMap(parse_record)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `ParDo` transform requires a bit more code but is essential if the element-wise processing does more than just manipulate data. We will not cover these use cases in this notebook. We just show the basic steps of defining a `DoFn` and use it in a `ParDo` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParseRecordDoFn(df.DoFn):\n",
    "    def process(self, context):\n",
    "        import json\n",
    "        r = json.loads(context.element)\n",
    "        yield r['ProductID'], r['Price']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\r\n",
      "(2, 1000)\r\n",
      "(3, 1000)\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.ParDo(ParseRecordDoFn())\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's aggregate the sales for each product ID. Dataflow contains lots of combiner functions defined in the [combiners.py](https://github.com/GoogleCloudPlatform/DataflowPythonSDK/blob/master/google/cloud/dataflow/transforms/combiners.py) module. As with ParDo transforms we allow Python function to be used as combiner functions as longs as they are commutative and associative.\n",
    "The grouping and combining transforms typically expect key-value pairs. In Python Dataflow these are simply represented as 2-tuples (e.g., ('a key', 'a value'), (1234, 'value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 100000)\r\n",
      "(8, 100000)\r\n",
      "(5, 100000)\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.FlatMap(parse_record)\n",
    " | df.CombinePerKey(sum)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions used in Map and FlatMap are not limited to just one parameter. They can have an arbitrary number of positional and keyword arguments evaluated when the transform gets created. For example, let's say the `parse_record` function has a `filtered` argument for products that should be skipped.\n",
    "\n",
    "__Note__. Each argument expression is evaluated at pipeline construction time _once_. Do not write code with the expectation that the argument expression is evaluated per function call or for cloud execution per worker startup. For example if the argument expression is `random.randint` then the same random number will be used for all calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "def parse_record(e, filtered):\n",
    "    import json\n",
    "    r = json.loads(e)\n",
    "    if int(r['ProductID']) not in filtered:\n",
    "        yield r['ProductID'], r['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100000)\r\n"
     ]
    }
   ],
   "source": [
    "%%dataflow_run\n",
    "(df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.FlatMap(parse_record, filtered)\n",
    " | df.CombinePerKey(sum)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra arguments to Map/FlatMap functions can be also _deferred_. Such arguments are specified by applying a _view_ operation to a PCollection. The possible view operations are AsSingleton, AsIter, AsList, and AsDict. They are all defined in [pvalue.py](https://github.com/GoogleCloudPlatform/DataflowPythonSDK/blob/master/google/cloud/dataflow/pvalue.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.cloud.dataflow import pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100000)\r\n"
     ]
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "filtered_pcoll = p | df.Create(filtered)\n",
    "(p \n",
    " | df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.FlatMap(parse_record, pvalue.AsIter(filtered_pcoll))\n",
    " | df.CombinePerKey(sum)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))\n",
    "\n",
    "p.run()\n",
    "!head -3 $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing graphs do not have to be linear. They are actually directed acyclic graphs (DAGs). Below is a workflow that reads the input file twice and then aggregates the results. The transform instances applied in a workflow must have unique labels. The framework generates automatically labels but in some cases, like the two `Read` transforms below, an additional string argument with the label must be passed in. All `PTransform` constructors will take an optional label argument (first position). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 200000)\r\n",
      "(8, 200000)\r\n",
      "(5, 200000)\r\n"
     ]
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "pc1 = p | df.io.Read('Read once', df.io.TextFileSource(input_file)) \n",
    "pc2 = p | df.io.Read('Read twice', df.io.TextFileSource(input_file)) \n",
    "\n",
    "((pc1, pc2) \n",
    " | df.Flatten()\n",
    " | df.FlatMap(parse_record, filtered=[])\n",
    " | df.CombinePerKey(sum)\n",
    " | df.io.Write(df.io.TextFileSink(output_file)))\n",
    "\n",
    "p.run()\n",
    "!head -3 $output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write now the resulting aggregation in a BigQuery table. You can read more about BigQuery here: https://cloud.google.com/bigquery/what-is-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_table = 'silviuc-dataflow:demo.silviuc_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change two things. First, we replace the sink used in the `Write` transform with a `BigQuerySink`. Second, we need to insert a Map transform before the write to convert the resulting tuple values to a the dictionary format expected by the BigQuery sink. The link to the resulting table is printed when the workflow is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting BigQuery table:\n",
      "https://bigquery.cloud.google.com/table/silviuc-dataflow:demo.silviuc_demo?pli=1\n"
     ]
    }
   ],
   "source": [
    "p = df.Pipeline('DirectPipelineRunner')\n",
    "(p \n",
    " | df.io.Read(df.io.TextFileSource(input_file))\n",
    " | df.FlatMap(parse_record, pvalue.AsIter(filtered_pcoll))\n",
    " | df.CombinePerKey(sum)\n",
    " | df.Map(lambda (pr, v): {'ProductID': pr, 'Value': v}) \n",
    " | df.io.Write(df.io.BigQuerySink(\n",
    "            output_table,\n",
    "            schema='ProductID:INTEGER, Value:FLOAT', \n",
    "            create_disposition=df.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=df.io.BigQueryDisposition.WRITE_TRUNCATE))) \n",
    "\n",
    "p.run()\n",
    "print 'Resulting BigQuery table:'\n",
    "print 'https://bigquery.cloud.google.com/table/%s?pli=1' % output_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (finally!) let's run the same workflow in the Google Cloud using the Dataflow [service](https://cloud.google.com/dataflow/). We will need to do a few more preparations. First, we need to use a Google Cloud project to submit the workflow. Second, we need to have the input files staged somewhere in Google Cloud Storage. Jobs running in the Google Cloud will not be able to access files stored locally. All the data must be accessible from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_name = 'silviuc-demo'\n",
    "project = 'silviuc-dataflow'\n",
    "input_files = 'gs://silviuc-dataflow/demo/events*'\n",
    "staging_location = 'gs://silviuc-dataflow/demo/staging'\n",
    "temp_location = 'gs://silviuc-dataflow/demo/temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cloud execution, we create a `Pipeline` object with the following arguments:\n",
    "* `--job_name`: Arbitrary name for the job executing the workflow.\n",
    "* `--project`: Name of the Google Cloud project used to run the job.\n",
    "* `--staging_location`: A Google Cloud Storage location for staging files required to execute the workflow.\n",
    "* `--temp_location`: A Google Cloud Storage location for temporary files created during workflow execution.\n",
    "* `--no_save_main_session`: Required when running inside a IPython notebook to avoid pickling errors.\n",
    "* `--runner`: The Google Cloud runner for the job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = df.Pipeline(argv=[\n",
    "        '--job_name', job_name,\n",
    "        '--project', project, \n",
    "        '--staging_location', staging_location, \n",
    "        '--temp_location', temp_location,\n",
    "        '--no_save_main_session',\n",
    "        '--runner', 'BlockingDataflowPipelineRunner',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow code does not change at all between a local run and a cloud run. Note that the source sink uses the variable `input_files` to reflect that we use a glob pattern to specify more files as input. The glob pattern can also be used for local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow workflow execution at: https://console.cloud.google.com/dataflow?project=silviuc-dataflow\n",
      "Resulting BigQuery table:\n",
      "https://bigquery.cloud.google.com/table/silviuc-dataflow:demo.silviuc_demo?pli=1\n"
     ]
    }
   ],
   "source": [
    "(p \n",
    " | df.io.Read(df.io.TextFileSource(input_files))\n",
    " | df.FlatMap(parse_record, pvalue.AsIter(filtered_pcoll))\n",
    " | df.CombinePerKey(sum)\n",
    " | df.Map(lambda (pr, v): {'ProductID': pr, 'Value': v}) \n",
    " | df.io.Write(df.io.BigQuerySink(\n",
    "            output_table,\n",
    "            schema='ProductID:INTEGER, Value:FLOAT', \n",
    "            create_disposition=df.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=df.io.BigQueryDisposition.WRITE_TRUNCATE))) \n",
    "\n",
    "print 'Follow workflow execution at: %s' % (\n",
    "    'https://console.cloud.google.com/dataflow?project=%s' % project)\n",
    "p.run()\n",
    "print 'Resulting BigQuery table:'\n",
    "print 'https://bigquery.cloud.google.com/table/%s?pli=1' % output_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
